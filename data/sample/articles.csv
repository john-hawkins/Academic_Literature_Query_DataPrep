Record number,Author,Year,Title,Reference type,Journal,Volume,Pages,Abstract,Keywords,Accesion number
1,Ashutosh Adhikari-//-Achyudh Ram-//-Raphael Tang-//-Jimmy Lin,2019,DocBERT: BERT for Document Classification,preprint,CoRR,1904.08398,,"We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",document classification,1904.08398
2,Giannis Nikolentzos-//-Antoine J. -P. Tixier-//-Michalis Vazirgiannis,2019,Message Passing Attention Networks for Document Understanding,preprint,CoRR,1908.06267,,"Graph neural networks have recently emerged as a very effective framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks. Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose several hierarchical variants of MPAD. Experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the different components on performance. Code is publicly available at",text mining,1908.06267
3,Yifan Peng-//-Shankai Yan-//-Zhiyong Lu,2019,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,Conference,BioNLP,,,"Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at",natural language processing,1906.05474
4,Stephan Gouws-//-Yoshua Bengio-//-Greg Corrado,2014,BilBOWA: Fast Bilingual Distributed Representations without Word Alignments,preprint,CoRR,,,"We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.",text mining,1410.2455
5,Matheus Werner-//-Eduardo Laber,2019,Speeding up Word Mover's Distance and its variants via properties of distances between embeddings,Conference,ECAI2020,,,"The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance between documents that takes advantage of semantic relations among words that are captured by their embeddings. This distance proved to be quite effective, obtaining state-of-art error rates for classification tasks, but is also impracticable for large collections/documents due to its computational complexity. For circumventing this problem, variants of WMD have been proposed. Among them, Relaxed Word Mover's Distance (RWMD) is one of the most successful due to its simplicity, effectiveness, and also because of its fast implementations.
Relying on assumptions that are supported by empirical properties of the distances between embeddings, we propose an approach to speed up both WMD and RWMD. Experiments over 10 datasets suggest that our approach leads to a significant speed-up in document classification tasks while maintaining the same error rates.",text mining,1912.00509
6,Konstantinos Skianis-//-Giannis Nikolentzos-//-Stratis Limnios-//-Michalis Vazirgiannis,2019,Rep the Set: Neural Networks for Learning Set Representations,Conference,AISTATS 2020,,,"In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms.",text mining,1904.01962
7,Ashutosh Adhikari-//-Achyudh Ram-//-Raphael Tang-//-Jimmy Lin,2019,Rethinking Complex Neural Network Architectures for Document Classification,Conference,ACL 2019,,4046Ð4051,"Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.",document classification,N19-1408
8,Tyler Dauphinee-//-Nikunj Patel-//-Mohammad Rashidi,2019,Modular Multimodal Architecture for Document Classification,preprint,CoRR,,,"Page classification is a crucial component to any document analysis system, allowing for complex branching control flows for different components of a given document. Utilizing both the visual and textual content of a page, the proposed method exceeds the current state-of-the-art performance on the RVL-CDIP benchmark at 93.03% test accuracy.",document classification,1912.04376
9,Sean Massung-//-Chase Geigle-//-ChengXiang Zhai,2016,MeTA: A Unified Toolkit for Text Retrieval and Analysis,Conference,NAACL,,91Ð96,"META is developed to unite machine learning, information retrieval, and natural language processing in one easy-to-use toolkit. Its focus on indexing allows it to perform well on large datasets, supporting online classification and other out-of-core algorithms. METAÕs liberal open source license encourages contributions, and its extensive online documentation, forum, and tutorials make this process straightforward. We run experiments and show METAÕs performance is competitive with or better than existing software.",text mining,P16-4016
10,Zichao Yang-//-Diyi Yang-//-Chris Dyer-//-Xiaodong He-//-Alex Smola-//-Eduard Hovy,2016,Hierarchical Attention Networks for Document Classification,Conference,NAACL,,1480Ð1489,"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.",document classification -//-text mining,N16-1174
